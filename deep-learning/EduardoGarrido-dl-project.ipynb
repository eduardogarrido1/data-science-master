{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmXtbmV_51Js"
   },
   "source": [
    "## 1. Carga y exploración de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skfdWZIG4Xa8"
   },
   "outputs": [],
   "source": [
    "# Import de las principales librerías a utilizar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfFk0Mcp5eBY",
    "outputId": "4b66e718-e31b-472f-a859-9cbdbd16aeda"
   },
   "outputs": [],
   "source": [
    "# Descarga del Dataset\n",
    "!wget -O \"airbnb-listings-extract.csv\" \"https://public.opendatasoft.com/explore/dataset/airbnb-listings/download/?format=csv&disjunctive.host_verifications=true&disjunctive.amenities=true&disjunctive.features=true&refine.country=Spain&q=Madrid&timezone=Europe/London&use_labels_for_header=true&csv_separator=%3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "ub0jMiwI5rkr",
    "outputId": "4aeab0db-946c-42ce-9b7c-e658de8bcfe7"
   },
   "outputs": [],
   "source": [
    "# Lectura de los primeros 5 registros\n",
    "airbnb = pd.read_csv('./airbnb-listings-extract.csv', sep=';', decimal='.')\n",
    "airbnb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0klSN0F7i-6"
   },
   "source": [
    "Podemos observar que aparentemente se ha cargado correctamente el dataset. Veamos el shape para asegurarnos que efectivamente obtuvimos todos los registros que esperábamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcpukX157P5h",
    "outputId": "930e6296-ce25-4665-aa70-8e87847d35ce"
   },
   "outputs": [],
   "source": [
    "# Shape del Dataset\n",
    "airbnb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VD9B6QgoiKO_"
   },
   "source": [
    "## 2. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGtzt7K476mB"
   },
   "source": [
    "Viendo que obtuvimos todos los registros esperados y que la descarga ocurrió sin problema, veamos cuales son los campos de los cuales disponemos. Esto para poder identificar a simple vista algunas variables que podrían no ser útiles para el modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqS60afX7YFI",
    "outputId": "3200691c-cdf2-4094-fda0-fcaf76a5198f"
   },
   "outputs": [],
   "source": [
    "# Campos del Dataset\n",
    "airbnb.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPN2YonF8Y2g"
   },
   "source": [
    "Muchas de la variables vistas no aportan información relevante, por lo que se eliminarán directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdZW-HTD8SQo"
   },
   "outputs": [],
   "source": [
    "# Eliminación de columnas no relevantes del Dataset\n",
    "airbnb = airbnb.drop(['ID',\n",
    "                      'Listing Url',\n",
    "                      'Scrape ID',\n",
    "                      'Last Scraped',\n",
    "                      'Medium Url',\n",
    "                      'Picture Url',\n",
    "                      'XL Picture Url',\n",
    "                      'Host ID',\n",
    "                      'Host URL',\n",
    "                      'Host Name',\n",
    "                      'Host Thumbnail Url',\n",
    "                      'Host Picture Url',\n",
    "                      'Host Neighbourhood',\n",
    "                      'Weekly Price',\n",
    "                      'Monthly Price',\n",
    "                      'Calendar Updated',\n",
    "                      'Calendar last Scraped',\n",
    "                      'First Review',\n",
    "                      'Last Review',\n",
    "                      'Reviews per Month',\n",
    "                      'Geolocation',\n",
    "                      'Calculated host listings count',\n",
    "                      'Host Listings Count',\n",
    "                      'Host Total Listings Count',\n",
    "                      'Security Deposit',\n",
    "                      'Cleaning Fee',\n",
    "                      'Name',\n",
    "                      'Summary',\n",
    "                      'Space',\n",
    "                      'Description',\n",
    "                      'Neighborhood Overview',\n",
    "                      'Notes',\n",
    "                      'Transit',\n",
    "                      'Access',\n",
    "                      'Interaction',\n",
    "                      'House Rules',\n",
    "                      'Host Location',\n",
    "                      'Host About'], axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-H09p6O9zgf"
   },
   "source": [
    "Ahora veamos que efectivamente se eliminaron las columnas no relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SA92fYA9zAS",
    "outputId": "4b8dc3aa-0b97-41e2-be8b-e250d66ba98d"
   },
   "outputs": [],
   "source": [
    "# Campos del Dataset\n",
    "airbnb.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erZJUmio_mbg"
   },
   "source": [
    "Veamos el tipo de dato de los campos y si existen registros vacíos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlCm54vM97qK",
    "outputId": "fa36050e-fce9-4862-b5fd-8ff935119dc1"
   },
   "outputs": [],
   "source": [
    "# Información relevante\n",
    "airbnb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwN-xJszAeRC"
   },
   "source": [
    "Podemos notar que hay muchos campos que aportan información duplicada, imprecisa o con demasiados registros vacíos. Esto definitivamente sería un problema para el modelado, por lo que vamos a proceder a eliminarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vh85qZvk_1k9"
   },
   "outputs": [],
   "source": [
    "# Eliminación campos con información duplicada, imprecisa o con demasiados registros vacíos\n",
    "airbnb = airbnb.drop(['Street',\n",
    "                      'State',\n",
    "                      'Market',\n",
    "                      'Smart Location',\n",
    "                      'Country',\n",
    "                      'Zipcode',\n",
    "                      'Host Acceptance Rate',\n",
    "                      'Square Feet',\n",
    "                      'Has Availability',\n",
    "                      'License',\n",
    "                      'Jurisdiction Names',\n",
    "                      'Review Scores Value',\n",
    "                      'Availability 30',\n",
    "                      'Availability 60',\n",
    "                      'Availability 90',\n",
    "                      'Review Scores Accuracy',\n",
    "                      'Review Scores Cleanliness',\n",
    "                      'Review Scores Checkin',\n",
    "                      'Review Scores Communication',\n",
    "                      'Review Scores Location',\n",
    "                      'Accommodates'], axis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uRCUwq-vDYCe",
    "outputId": "b7c2a2e3-d22d-45b7-c08d-bd7d89279d64"
   },
   "outputs": [],
   "source": [
    "# Campos del Dataset\n",
    "airbnb.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhYIxJM5IjCE"
   },
   "source": [
    "Veamos la información relevante de las variables que mantenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQSbuzLOIqSW",
    "outputId": "c5d337db-d561-4abd-fa0a-4bb1ff601d63"
   },
   "outputs": [],
   "source": [
    "airbnb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BmGRiFbJb1s"
   },
   "source": [
    "Ahora trabajaremos con los missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APJV2AZDJxL4"
   },
   "outputs": [],
   "source": [
    "# Vencindarios\n",
    "\n",
    "airbnb['Neighbourhood'] = airbnb['Neighbourhood Group Cleansed'].fillna(airbnb['Neighbourhood Cleansed'])\n",
    "airbnb = airbnb.drop(['Neighbourhood Group Cleansed', 'Neighbourhood Cleansed', 'City', 'Country Code'], axis=1)\n",
    "\n",
    "# Textos\n",
    "\n",
    "airbnb['Features'] = airbnb['Features'].fillna('')\n",
    "airbnb['Features'] = airbnb['Features'].apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "airbnb['Amenities'] = airbnb['Amenities'].fillna('')\n",
    "airbnb['Amenities'] = airbnb['Amenities'].apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "airbnb['Host Verifications'] = airbnb['Host Verifications'].fillna('')\n",
    "airbnb['Host Verifications'] = airbnb['Host Verifications'].apply(lambda x: len(str(x).split(',')))\n",
    "\n",
    "# Fechas\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "airbnb = airbnb.dropna(subset=['Host Since'])\n",
    "airbnb['Host Since'] = airbnb['Host Since'].apply(lambda x: datetime.strptime(str(x),'%Y-%m-%d'))\n",
    "airbnb['Host Since'] = airbnb['Host Since'].apply(lambda x: 2017-x.year)\n",
    "\n",
    "# Host\n",
    "\n",
    "airbnb['Host Response Time'] = airbnb['Host Response Time'].fillna(airbnb['Host Response Time'].mode()[0])\n",
    "airbnb = airbnb.drop('Experiences Offered', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94MbBYxmPGCk"
   },
   "source": [
    "Ahora verificamos nuestros campos nuevamente para asegurarnos que los cambios han sido aplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOhQUCtrPPbV",
    "outputId": "6703a4bf-9a48-4cec-e6cd-394bf6712352"
   },
   "outputs": [],
   "source": [
    "# Campos del Dataset\n",
    "airbnb.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4KBVmHxkE2m"
   },
   "source": [
    "## 3. Carga de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M4LL_KsEkIRB",
    "outputId": "61907460-91a5-4afd-d5fe-572f6fb3e0ed"
   },
   "outputs": [],
   "source": [
    "import imageio as io\n",
    "import numpy as np\n",
    "import cv2\n",
    "n_images = 800\n",
    "images = np.zeros((n_images, 224, 224, 3), dtype=np.uint8)\n",
    "urls = airbnb['Thumbnail Url']\n",
    "\n",
    "i_aux = 0\n",
    "good_urls = []\n",
    "for i_img, url in enumerate(urls):\n",
    "    if len(good_urls) >= n_images:\n",
    "        # ya tenemos n_images imágenes\n",
    "        break\n",
    "    try:\n",
    "        img = io.imread(url)\n",
    "        images[i_aux] = cv2.resize(img, (224, 224))\n",
    "        good_urls.append(i_img)\n",
    "        i_aux += 1\n",
    "        print(f'Imagen {i_img} descargada')\n",
    "        print(len(good_urls))\n",
    "    except IOError as err:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFVhoKWwq56T",
    "outputId": "50d0c908-0530-405c-991b-9efa5bd62225"
   },
   "outputs": [],
   "source": [
    "# Visualizamos las imágenes cargadas\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YakYj5I8rW0t",
    "outputId": "c317a317-9c72-48e3-925d-524c6e72f750"
   },
   "outputs": [],
   "source": [
    "# Mantenemos los datos numéricos solo para aquellos pisos que tienen imágenes \n",
    "# y las hemos obtenido\n",
    "airbnb = airbnb.iloc[good_urls, :]\n",
    "print(airbnb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiKz7pnarfp2"
   },
   "outputs": [],
   "source": [
    "# Obtener las etiquetas de regresion\n",
    "y_reg = airbnb['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b3X_e_Krn3_"
   },
   "outputs": [],
   "source": [
    "# guardamos las imágenes (y yo os recomiendo que os lo guardéis en GDrive para evitar tener que repetir esto)\n",
    "np.save('images.npy', images)\n",
    "np.save('final_data.npy', airbnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "sbbgWn0mru3_",
    "outputId": "3219ab7b-c167-49aa-9aaf-bfba3c94f9bf"
   },
   "outputs": [],
   "source": [
    "# y un rango para clasificación (del 1 al 3 por ejemplo: barato, normal, caro)\n",
    "plt.hist(y_reg, bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-vuH90Tr3x5"
   },
   "outputs": [],
   "source": [
    "y_class = []\n",
    "for x in y_reg:\n",
    "    # barato\n",
    "    if x <= 50:\n",
    "        y_class.append(0)\n",
    "    elif x <=150:\n",
    "        y_class.append(1)\n",
    "    else:\n",
    "        y_class.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "3A3y7cIjr-Wi",
    "outputId": "9dd6dc44-a267-4f93-b336-397908517b77"
   },
   "outputs": [],
   "source": [
    "# veamos cómo ha quedado la distribución al convertirla a 3 clases\n",
    "plt.hist(y_class, bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RVU3hd-sDV1"
   },
   "source": [
    "## 4. Normalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgF98QcA38Mg"
   },
   "source": [
    "Todavía quedan algunos campos con missing values, pero al ser pocos, podemos rellenarlos. Para este fin usaremos la media o la moda según corresponda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMz29Vi83aEx",
    "outputId": "1381a271-4204-48b7-bb19-e8b3dc2fe319"
   },
   "outputs": [],
   "source": [
    "# Rellenamos algunos missing values con la media\n",
    "airbnb['Host Response Rate'] = airbnb['Host Response Rate'].fillna(airbnb['Host Response Rate'].mean())\n",
    "airbnb['Review Scores Rating'] = airbnb['Review Scores Rating'].fillna(airbnb['Review Scores Rating'].mean())\n",
    "\n",
    "# Rellenamos algunos missing values con la moda\n",
    "airbnb['Bathrooms'] = airbnb['Bathrooms'].fillna(airbnb['Bathrooms'].mode())\n",
    "airbnb['Beds'] = airbnb['Beds'].fillna(airbnb['Beds'].mode())\n",
    "\n",
    "# Nos aseguramos que nuestro proceso ha ocurrido satisfactoriamiente\n",
    "airbnb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYnd7Oys7pk7"
   },
   "source": [
    "Ahora veremos cuales son nuestras variables categóricas para codificarlas mediante OneHotEnconding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dLCW99872E4",
    "outputId": "759979ea-9568-4d4f-ef05-76106ef3020f"
   },
   "outputs": [],
   "source": [
    "airbnb.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZtrB5O848K8l",
    "outputId": "9edd3095-b825-48ae-bfe1-cebbeb2db202"
   },
   "outputs": [],
   "source": [
    "# Aplicamos One Hot Encoding mediante get_dummies a todas las columnas de tipo object con excepción de 'Thumbnail Url'\n",
    "airbnb = pd.get_dummies(airbnb, columns=['Host Response Time',\n",
    "                                         'Neighbourhood',\n",
    "                                         'Property Type',\n",
    "                                         'Room Type',\n",
    "                                         'Bed Type',\n",
    "                                         'Cancellation Policy'])\n",
    "\n",
    "# Comprobamos que ha funcionado\n",
    "airbnb.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oyL-0Yq9faJ"
   },
   "source": [
    "Ahora procederemos a normalizar. Para esto utilizaremos la clase MaxMinScaler de Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NgZU9UF0sBDR",
    "outputId": "42dd162a-5904-4731-e268-eda8b37af321"
   },
   "outputs": [],
   "source": [
    "# Importamos la clase a utilizar\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Definimos la función normalizadora\n",
    "def normalizadora(campo):\n",
    "  CampoTransformar = airbnb[\"Host Response Rate\"]\n",
    "  CampoTransformar_nparray = CampoTransformar.values.reshape(-1, 1)\n",
    "  scaler = MinMaxScaler()\n",
    "  return scaler.fit_transform(CampoTransformar_nparray)\n",
    "\n",
    "# Aplicamos la función normalizadora a nuestro DataFrame\n",
    "airbnb[\"Host Response Rate\"] = normalizadora(\"Host Response Rate\")\n",
    "airbnb[\"Host Verifications\"] = normalizadora(\"Host Verifications\")\n",
    "airbnb[\"Latitude\"] = normalizadora(\"Latitude\")\n",
    "airbnb[\"Longitude\"] = normalizadora(\"Longitude\")\n",
    "airbnb[\"Bathrooms\"] = normalizadora(\"Bathrooms\")\n",
    "airbnb[\"Bedrooms\"] = normalizadora(\"Bedrooms\")\n",
    "airbnb[\"Beds\"] = normalizadora(\"Beds\")\n",
    "airbnb[\"Amenities\"] = normalizadora(\"Amenities\")\n",
    "airbnb[\"Guests Included\"] = normalizadora(\"Guests Included\")\n",
    "airbnb[\"Extra People\"] = normalizadora(\"Extra People\")\n",
    "airbnb[\"Minimum Nights\"] = normalizadora(\"Minimum Nights\")\n",
    "airbnb[\"Maximum Nights\"] = normalizadora(\"Maximum Nights\")\n",
    "airbnb[\"Availability 365\"] = normalizadora(\"Availability 365\")\n",
    "airbnb[\"Number of Reviews\"] = normalizadora(\"Number of Reviews\")\n",
    "airbnb[\"Features\"] = normalizadora(\"Features\")\n",
    "airbnb[\"Review Scores Rating\"] = normalizadora(\"Review Scores Rating\")\n",
    "airbnb[\"Host Since\"] = normalizadora(\"Host Since\")\n",
    "\n",
    "# Comprobamos que ha funcionado\n",
    "airbnb.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EmWwJONDoRu"
   },
   "source": [
    "## 5. Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meMDQ3TWBr6K"
   },
   "source": [
    "A partir de aquí dividiremos el DataFrame en dos versiones. Una que utilizaremos para trabajar un modelo basado en datos 1D y una que utilizaremos para trabajar un modelo basado en imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_Wk7qeizBq1"
   },
   "outputs": [],
   "source": [
    "# DataFrame para modelo basado en datos\n",
    "airbnb_datos = airbnb.drop('Thumbnail Url', axis=1)\n",
    "\n",
    "# DataFrame para modelo basado en imagenes\n",
    "airbnb_imagenes = airbnb.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nKj04qnDf_p"
   },
   "source": [
    "#### Primer módulo: Modelo basado en datos 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttbMjOgIERXx"
   },
   "source": [
    "Como buena práctica, convertiremos los datos del DataFrame con el cual vamos a trabajar a float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "TaupnI_cD24o",
    "outputId": "1cb246d4-b91a-4dca-d56f-d4bc0e0e62ba"
   },
   "outputs": [],
   "source": [
    "airbnb_datos = airbnb_datos.astype(np.float32)\n",
    "airbnb_datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMA19T3jEqBR"
   },
   "source": [
    "Ahora realizaremos el split en Train, Validation y Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1r6oz8qExwC"
   },
   "outputs": [],
   "source": [
    "# Importamos la clase necesaria de Scikit-Learn para hacer el split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividimos el DataFrame en df_train_val y df_test\n",
    "df_train_val, df_test = train_test_split(airbnb_datos, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dividiremos df_train_val en df_train y df_val\n",
    "df_train, df_val = train_test_split(df_train_val, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHkqheNkG4wQ"
   },
   "source": [
    "Ahora dividiremos nuestros DataFrames en variables predictoras y variable a predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiwB3EG8HBBu"
   },
   "outputs": [],
   "source": [
    "# df_train\n",
    "x_train = df_train.drop('Price', axis=1)\n",
    "y_train = df_train['Price']\n",
    "\n",
    "# df_val\n",
    "x_val = df_val.drop('Price', axis=1)\n",
    "y_val = df_val['Price']\n",
    "\n",
    "# df_test\n",
    "x_test = df_test.drop('Price', axis=1)\n",
    "y_test = df_test['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9bh6aBNH4yC"
   },
   "source": [
    "Por último, normalizaremos nuestra variable a predecir para los datos de train, validation y test. Esto con el fin de que el entrenamiento del modelo sea menos complejo en términos computacionales. Esto lo haremos dividiendo entre el precio máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bxMT7WVIdYR"
   },
   "outputs": [],
   "source": [
    "# Precio máximo\n",
    "maxPrice = airbnb_datos['Price'].max()\n",
    "\n",
    "# Normalización variable a predecir\n",
    "y_train = y_train / maxPrice\n",
    "y_val = y_val / maxPrice\n",
    "y_test = y_test / maxPrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KIMBLe4LY88"
   },
   "source": [
    "Ahora procederemos a crear el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EE98ZoRoL7an",
    "outputId": "2a952c72-a53e-414d-f74a-04afd05e2dc7"
   },
   "outputs": [],
   "source": [
    "# Importamos las clases a utilizar\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# Definimos los hiperparámetros que utilizaremos\n",
    "LearningRate = 0.05\n",
    "NumeroEpocas = 100\n",
    "\n",
    "# Definimos la arquitectura\n",
    "model_1D = Sequential()\n",
    "model_1D.add(Dense(32, activation='relu', input_dim=x_train.shape[1]))\n",
    "model_1D.add(Dense(16, activation='relu'))\n",
    "model_1D.add(Dense(8, activation='relu'))\n",
    "model_1D.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilamos y ajustamos el modelo\n",
    "model_1D.compile(loss='mse', optimizer=SGD(LearningRate), metrics=['mse'])\n",
    "H = model_1D.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=NumeroEpocas, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5QpOUWK81VX"
   },
   "source": [
    "Ahora veamos como son los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "kM3Rmlr1848q",
    "outputId": "77a5eb64-bbe4-4c2b-fc91-a469b5a9d2ef"
   },
   "outputs": [],
   "source": [
    "# Primero calculamos las predicciones\n",
    "y_pred_train = model_1D.predict(x_train)\n",
    "y_pred_val = model_1D.predict(x_val)\n",
    "y_pred_test = model_1D.predict(x_test)\n",
    "\n",
    "# Ahora desnormalizamos las predicciones\n",
    "y_pred_train_desnorm = y_pred_train[:, 0] * maxPrice\n",
    "y_pred_val_desnorm = y_pred_val[:, 0] * maxPrice\n",
    "y_pred_test_desnorm = y_pred_test[:, 0] * maxPrice\n",
    "\n",
    "# Luego desnormalizamos los valores reales\n",
    "y_train_desnorm = y_train * maxPrice\n",
    "y_val_desnorm = y_val * maxPrice\n",
    "y_test_desnorm = y_test * maxPrice\n",
    "\n",
    "# Importamos la clase que nos permitirá medir los errores\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculamos los errores\n",
    "rmse_train = mean_squared_error(y_train_desnorm, y_pred_train_desnorm, squared=False)\n",
    "rmse_val = mean_squared_error(y_val_desnorm, y_pred_val_desnorm, squared=False)\n",
    "rmse_test = mean_squared_error(y_test_desnorm, y_pred_test_desnorm, squared=False)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(f'RMSE train: {rmse_train}')\n",
    "print(f'RMSE val: {rmse_val}')\n",
    "print(f'RMSE test: {rmse_test}')\n",
    "\n",
    "# Hacemos la gráfica\n",
    "epocas = range(1, len(H.history['loss']) + 1)\n",
    "plt.plot(epocas, H.history['loss'])\n",
    "plt.plot(epocas, H.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model 1D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0v5uuw-HPhG"
   },
   "source": [
    "Podemos ver que los valores obtenidos por train, validation y test son cercanos, por lo que podemos concluir que no estamos sufriendo de Overfitting. De igual forma, podemos observar que la gráfica muestra cercania entre los valores de train y validation. Con esto podemos confirmar que la predicción es capaz de darnos valores razonablemente cercanos a la realidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWIIH10_Hs-x"
   },
   "source": [
    "#### Primer módulo: Modelo basado en imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lsy8ZSwKh3B"
   },
   "source": [
    "En primer lugar, definamos nustras variables predictoras y nuestra variable a predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPUdIFNSH0v2"
   },
   "outputs": [],
   "source": [
    "dfx = images\n",
    "dfy = airbnb_imagenes['Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZlKyoQ8Lri6"
   },
   "source": [
    "Ahora realizaremos el split en Train, Validation y Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6bWm-ewMHC1"
   },
   "outputs": [],
   "source": [
    "# Importamos la clase necesaria de Scikit-Learn para hacer el split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividimos los DataFrames dfx y dfy en x_train_val, x_test, y_train_val y y_test\n",
    "x_train_val_images, x_test_images, y_train_val_images, y_test_images = train_test_split(dfx, dfy, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dividiremos los DataFrames x_train_val y y_train_val en x_train, x_val, y_train y y_val\n",
    "x_train_images, x_val_images, y_train_images, y_val_images = train_test_split(x_train_val_images,\n",
    "                                                                              y_train_val_images,\n",
    "                                                                              test_size=0.1,\n",
    "                                                                              random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25XJc0w5OzO5"
   },
   "source": [
    "Por último, normalizaremos nuestras variables predictoras y nuestras variables a predecir. Esto con el fin de que el entrenamiento del modelo sea menos complejo en términos computacionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mfan552CPOQ9"
   },
   "outputs": [],
   "source": [
    "# Precio máximo\n",
    "maxPrice = airbnb_datos['Price'].max()\n",
    "\n",
    "# Normalización variable a predecir\n",
    "y_train_images = y_train_images / maxPrice\n",
    "y_val_images = y_val_images / maxPrice\n",
    "y_test_images = y_test_images / maxPrice\n",
    "\n",
    "# Normalización variables predictoras\n",
    "x_train_images = x_train_images.astype('float32') / 255.0\n",
    "x_val_images = x_val_images.astype('float32') / 255.0\n",
    "x_test_images = x_test_images.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzyFd0BzP7BA"
   },
   "source": [
    "Ahora procedemos a crear el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bp2qQzdkP-RS",
    "outputId": "52102341-15b3-422f-96e7-7627c472abe2"
   },
   "outputs": [],
   "source": [
    "# Importamos las clases a utilizar\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Definimos la arquitectura\n",
    "model_images = Sequential()\n",
    "model_images.add(Conv2D(256, (3,3), activation='relu', input_shape=(224,224,3)))\n",
    "model_images.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_images.add(Dropout(0.25))\n",
    "model_images.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model_images.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_images.add(Dropout(0.25))\n",
    "model_images.add(Conv2D(16, (3,3), activation='relu'))\n",
    "model_images.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_images.add(Dropout(0.25))\n",
    "model_images.add(Flatten())\n",
    "model_images.add(Dense(8, activation='relu'))\n",
    "model_images.add(Dropout(0.25))\n",
    "model_images.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilamos y ajustamos el modelo\n",
    "model_images.compile(optimizer=Adam(learning_rate=0.05), loss='mse', metrics=['mse'])\n",
    "H = model_images.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val), shuffle=True)\n",
    "\n",
    "# Evaluamos el modelo\n",
    "scores = model_images.evaluate(x_test_images, y_test_images, verbose=1)\n",
    "print(f'Test Loss: {scores[0]}')\n",
    "print(f'Test Accuracy: {scores[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN9wZxSfaGvg"
   },
   "source": [
    "Ahora veamos como son los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQbhfkbAaRFR"
   },
   "outputs": [],
   "source": [
    "# Primero calculamos las predicciones\n",
    "y_pred_train_images = model_images.predict(x_train_images)\n",
    "y_pred_val_images = model_images.predict(x_val_images)\n",
    "y_pred_test_images = model_images.predict(x_test_images)\n",
    "\n",
    "# Ahora desnormalizamos las predicciones\n",
    "y_pred_train_images_desnorm = y_pred_train_images[:, 0] * maxPrice\n",
    "y_pred_val_images_desnorm = y_pred_val_images[:, 0] * maxPrice\n",
    "y_pred_test_images_desnorm = y_pred_test_images[:, 0] * maxPrice\n",
    "\n",
    "# Luego desnormalizamos los valores reales\n",
    "y_train_images_desnorm = y_train_images * maxPrice\n",
    "y_val_images_desnorm = y_val_images * maxPrice\n",
    "y_test_images_desnorm = y_test_images * maxPrice\n",
    "\n",
    "# Importamos la clase que nos permitirá medir los errores\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculamos los errores\n",
    "rmse_train = mean_squared_error(y_train_images_desnorm, y_pred_train_images_desnorm, squared=False)\n",
    "rmse_val = mean_squared_error(y_val_images_desnorm, y_pred_val_images_desnorm, squared=False)\n",
    "rmse_test = mean_squared_error(y_test_images_desnorm, y_pred_test_images_desnorm, squared=False)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(f'RMSE train: {rmse_train}')\n",
    "print(f'RMSE val: {rmse_val}')\n",
    "print(f'RMSE test: {rmse_test}')\n",
    "\n",
    "# Hacemos la gráfica\n",
    "epocas = range(1, len(H.history['loss']) + 1)\n",
    "plt.plot(epocas, H.history['loss'])\n",
    "plt.plot(epocas, H.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRqD-tTtq4XZ"
   },
   "source": [
    "Estos resultados son un poco desfavorables. Sin embargo, existe mucho campo de mejora si se ajustan los hiperparámetros o la arquitectura de la red neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlTNlrn_rOgP"
   },
   "source": [
    "#### Segundo módulo: Modelo híbrido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w69ZTW_VrdxV"
   },
   "source": [
    "Ahora procederemos a construir un modelo híbrido. Esto se hará con base en el modelo de datos 1D y en el modelo de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk9HbjmvrHj7"
   },
   "outputs": [],
   "source": [
    "# En primer lugar, importamos las clases a utilizar\n",
    "from tensorflow.keras import optimizers, Model\n",
    "from tensorflow.keras.layers import concatenate, Input, GlobalMaxPool2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Modelo 1D\n",
    "modelo1D = Dense(8, activation='relu')(Input(shape=x_train.shape[1]))\n",
    "modelo1D = Dense(4, activation='relu')(modelo1D)\n",
    "modelo1D = Model(inputs=Input(shape=x_train.shape[1]), outputs=modelo1D)\n",
    "\n",
    "# Preparación Modelo basado en imágenes\n",
    "pre_modelo_imagenes = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "for layer in pre_modelo_imagenes.layers:\n",
    "  if layer.name == 'block5_conv1':\n",
    "    break\n",
    "  layer.trainable = False\n",
    "\n",
    "# Modelo basado en imágenes\n",
    "modelo_imagenes = GlobalMaxPool2D(name='GlobalMaxPool')(pre_modelo_imagenes.layers[-1].output)\n",
    "modelo_imagenes = Dense(16, activation='relu', name='fc1')(modelo_imagenes)\n",
    "modelo_imagenes = Dense(8, activation='relu', name='fc2')(modelo_imagenes)\n",
    "modelo_imagenes = Model(inputs=pre_modelo_imagenes.inputs, outputs=modelo_imagenes)\n",
    "\n",
    "# Combinación de modelos\n",
    "modelo_combinado = concatenate([modelo1D.output, modelo_imagenes.output])\n",
    "capa = Dense(2, activation='relu')(modelo_combinado)\n",
    "capa = Dense(1, activation='sigmoid')(capa)\n",
    "HybridModel = Model(inputs=[modelo1D.input, modelo_imagenes.input], outputs=capa)\n",
    "\n",
    "# Compilación y ajuste del modelo\n",
    "HybridModel.compile(optimizer=Adam(learning_rate=0.005), loss='mse', metrics=['mse'])\n",
    "H = HybridModel.fit([x_train, x_train_images], y_train, epochs=20, batch_size=64, validation_data=([x_val, x_val_images], y_val))\n",
    "\n",
    "# Graficamos las pérdidas\n",
    "epocas = range(1, len(H.history['loss']) + 1)\n",
    "plt.plot(epocas, H.history['loss'])\n",
    "plt.plot(epocas, H.history['val_loss'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Hybrid Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgI5trOKfP-f"
   },
   "source": [
    "Podemos concluir que el potencial que poseen las redes neuronales es muy interesante. Actualmente, los únicos problemas que estos presentan son una complejidad computacional muy elevada y un largo tiempo de espera para entrenamiento. Si bien es cierto que esto es una desventaja para el Deep Learning, la tecnología va en mejora y, más pronto que tarde, los equipos serán más potentes y asequibles. Eventualmente la complejidad computacional no será un problema."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
